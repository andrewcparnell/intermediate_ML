---
title: 'Class 7: Anomaly Detection'
author: Andrew Parnell \newline \texttt{andrew.parnell@mu.ie}   \newline \vspace{1cm}
  \newline \includegraphics[width=3cm]{maynooth_uni_logo.jpg}
  \newline PRESS RECORD 
  \newline \vspace{1cm}
  \newline https://andrewcparnell.github.io/intermediate_ML
output:
  beamer_presentation:
    includes:
      in_header: header.tex
classoption: "aspectratio=169"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf', fig.height = 5)
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
library(tidyverse)
```

## Introduction to Anomaly Detection (AD)

- Anomaly detection refers to the process of identifying data points, observations, or patterns that deviate significantly from the norm or standard in a dataset. These  might be critical incidents, such as errors, fraud, or system failures
- What you do with an anomaly will depend on the application; you might remove it, stop the whole experiment, or just ignore and note it down for later evaluation
- We will explore lots of different methods in R for performing anomaly detection with examples for all of them

## Anomaly Detection in R

- We will use 3 main packages for performing AD, and discuss some other popular methods

  1. `anomalize`: Implements a tidy AD algorithm that works well with dplyr and tidyr pipelines. Pretty user friendly (https://www.youtube.com/watch?v=Gk_HwjhlQJs)
  2. `tsoutliers`: Use for detecting outliers in time-series data. Integrates well with ARIMA modeling and other time-series forecasting methods, though pretty slow
  3. `stray`: Designed for AD in high-dimensional data. More aligned with machine learning techniques as it uses projection and clustering
  
- We mostly focus on finding anomalies in time series but AD can be used on any type of data
  
## A simple example: CRAN downloads from `anomalize`

```{r, eval = FALSE}
library(anomalize)
tidyverse_cran_downloads %>%
    filter(package == "dplyr") %>%
    ungroup() %>%
    time_decompose(count, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE)
```

## A simple example: CRAN downloads from `anomalize` - plot

```{r, echo = FALSE, message=FALSE, results='hide'}
library(anomalize)
tidyverse_cran_downloads %>%
    filter(package == "dplyr") %>%
    ungroup() %>%
    time_decompose(count, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE)
```

## Types of Anomalies

- Point Anomalies. A single 'anomalous' data point. But beware, a sudden spike in e.g. energy usage on a hot day might be normal, but the same spike on a mild day could be anomalous.
- Collective Anomalies. A collection of 'anomalous' data points occurring together.
- Seasonal Anomalies. Anomalies that occur in a seasonal pattern within time-series data.
- Network Anomalies. Anomalies that occur in multiple time series simultaneously that might not be visible in a single series. 

## Anomaly Detection Methods

Often a time series model is fitted first and the anomaly detection routine is run on the leftover data (residuals).

- Basic methods: Z-scores, Inter-Quartile Range, Grubbs test, Control charts
- Statistical methods: Generalised-Extreme Studentised Deviate Test (GESD), Seasonal Hybrid ESD approach (Twitter / SH-ESD), Extreme value approaches
- Machine learning approaches. Dimension reduction approaches (stray), RNNs (followed by statistical methods)
- Hybrid methods. Using combinations of the above

## Decomposition and Anomaly Detection

Common to perform Seasonal Trend and irregular decomposition using Loess (STL) and running AD on the components:

```{r, echo = FALSE}
library(anomalize)
decomposed_data <- tidyverse_cran_downloads %>%
  filter(package == 'dplyr') %>% 
  time_decompose(count, method = "stl", frequency = "auto", trend = "auto")
decomposed_data %>% pivot_longer(-c(package, date),
                                 names_to = 'Component',
                                 values_to = 'Value') %>% 
  ggplot(aes(x = date, y = Value)) +
  geom_line() +
  facet_grid(Component ~ .) +
  labs(title = "STL Decomposition of dplyr Downloads",
     x = "Date", y = "Count")
```

## Basic Methods: G-ESD

- Iteratively tests for and removes the most extreme value as an outlier, allowing detection of multiple outliers in a dataset, as opposed to Grubbs test
- Uses a studentised range statistic, supposedly a robust method for identifying outliers in normally distributed data
- Effective for both small and large data sets, with an upper limit on the number of outliers found

Can be applied to a data set directly, but more commonly applied to the residuals. Provides a list of potential values up to the maximum number of anomalies allowed

## Seasonal Hybrid ESD approach

- SH-ESD is an extension of the GESD test specifically for seasonal data by performing a seasonal decomposition and windowing of the data before running the GESD test
- The decomposition allows for the detection of seasonal anomalies, which are extreme relative to a particular season or time frame but might not be extreme in the overall dataset
- Not treated as a particular anomaly detection method but rather a decomposition method in `anomalize`

(Recently extended to work on streaming data rather than just windows)

## Time Series Anomaly Detection with `tsoutliers`

Perhaps the most basic time series AD package is `tsoutliers`

- Package works by fitting ARIMA models to the data and then looking at different types of outliers
- Produces predictions and also potential adjustments to the data that would remove the outliers and make the data set `cleaner'
- Allows for ARIMAX type data (e.g. time series data with extra regressors)

## Types of outlier identified by `tsoutliers`

- Additive Outliers (AO), Innovational Outliers (IO), Level Shifts (LS), Temporary Changes (TC), and Seasonal Level Shifts (SLS).
- AO are sudden, abnormal spikes or drops in the time series that are not part of the usual pattern or trend.
- IO are irregularities that introduce a shock to the system, affecting the time series values both at the occurrence and subsequent periods.
- LS are sudden, lasting change in the level of the time series, reflecting a structural change in the process.
- TC are short-term anomalies where the time series deviates from its usual pattern for a brief period before returning to normal.
- SLS are similar to LS but occur in a seasonal pattern, indicating a permanent change in the seasonal component of the time series.

AO, IO and LS are the defaults looked for

## Revision: ARIMA models

- Combination of AR and MA: ARIMA models blend AutoRegressive (AR) and Moving Average (MA) approaches, where AR models leverage past values and MA models use past forecast errors for prediction
- Integration for Non-Stationarity: The 'I' in ARIMA stands for 'Integrated' and involves differencing the data to achieve stationarity, essential for time series forecasting
- Parameter Specification: Characterized by three parameters (p, d, q) - 'p' for the order of the AR part, 'd' for the degree of differencing, and 'q' for the order of the MA part
- Flexibility and Adaptability: Suitable for a wide range of time series data, capable of modeling various patterns and structures in both stationary and non-stationary data

Can also have seasonal ARIMA models

## Example of using tsoutliers

\tiny

```{r, warning=FALSE}
library(tsoutliers)
data(hicp) # Multivariate time series
tso(y = log(hicp$`011300`))
```

## Example of `anomalize` package

```{r, eval = FALSE, message=FALSE}
library(anomalize)
tidyverse_cran_downloads %>%
    filter(package == "dplyr") %>%
    ungroup() %>%
    time_decompose(count, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE)
```

## Output of `anomalize`

```{r, echo = FALSE, results = 'hide', message=FALSE}
tidyverse_cran_downloads %>%
    filter(package == "tidyquant") %>%
    ungroup() %>%
    time_decompose(count, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE)
```

## Machine Learning Methods: `stray` approach

- stray = STReam AnomalY. Ideal for high dimensional data
- Uses $k$-nearest neighbours to find the distances between the observations in a high dimensional space
- Uses ideas from Extreme value theory (EVT) to define a threshold and identify the gaps between the observations
- Enables the method to capture both 'in-liers' and outliers
- Produces both a list of outliers and an outlier score for further analysis

## Example of using the stray package

Very fast and pretty good

```{r, eval = FALSE}
library(stray)
set.seed(123)
multivariate_data <- cbind(rnorm(100), rnorm(100))
results <- find_HDoutliers(multivariate_data,
                           alpha = 0.2, # Tail value for outliers
                           k = 10) # Number of neighbours for knn 
most_outlying <- which.max(results$out_scores) # Useful to see 
multivariate_data[(most_outlying-2):(most_outlying+2),]
```

## Other machine learning approaches

Lots of other unsupervised approaches are used before running an AD algorithm

- K-means and mixture models (covered yesterday) commonly used
- DBSCAN is a spatial clustering algorithm that differentiates between core points (inside a cluster), border points (on the edge of a cluster), and noise points (isolated, outlier points)
- Isolation Forests is a version of random forests that spots data points that are commonly split at the top of a tree and are therefore likely to be outliers (R package `isotree`)

## Summary

- Lots of different AD techniques; most based on quite traditional statistical methods
- Usually perform a time series analysis, which can be quite simple or very complex, before running the AD algorithm
- Lots of different types of anomaly which may or may not be appropriate to different data problems
- See the examples in the script folder for more details on what these packages can do

