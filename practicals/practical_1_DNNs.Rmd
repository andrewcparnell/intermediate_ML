---
title: 'Day 1: Self-guided practical - Fitting DNNs using keras in R'
author: "Andrew Parnell"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
  pdf_document: default
---

```{r setup, include=FALSE}
rm(list=ls()) # Clear the workspace
knitr::opts_chunk$set(echo = TRUE)
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01, las=1)
options(width = 50)
pkgs = c('R2jags','rjags', 'lubridate', 'tidyverse','forecast', 'rstan')
lapply(pkgs, library, character.only = TRUE)

```

## Introduction

Welcome to the first user-guided practical on fitting deep neural networks in R with keras. In this practical you will:

- Fit some basic NNs using keras
- Explore the different options for how the NNs are constructed
- Compare the performance on some real data sets
- Create some ggplots of the output

There are three sections. You should work your way through the questions and put your hand up if you get stuck. There is an answer script given at the bottom but please try not to use it unless you are completely stuck and cannot ask a question.

***

You can run the code from these practicals by loading up the `.Rmd` (Rmarkdown) file in the same directory in Rstudio. Feel free to add in your own answers, or edit the text to give yourself extra notes. You can also run the code directly by highlighting the relevant code and clicking `Run`. Much of this material overlaps with the class slides so sometimes if you get stuck you might get a clue by looking at the `.Rmd` file in the `slides` folder.

One final small note: if you are copying R commands from the pdf or html files into your R script or console window sometimes the inverted commas can copy across incorrectly. If you get a weird message saying `Error: unexpected input` you usually just need to delete/replace the inverted commas.


## Task set 1: Fitting different types of neural network with keras in R

The data set `dataset_boston_housing()` is included in the `keras` R package. Start a script and load in the `tidyverse` and `keras` packages. 

Tasks:

1. Load in the data set and check that you understand the structure of the dataset using `glimpse` or `str`. What are the dimensions of the training and test data sets? 

1. Scale the training and test set data. Create some simple ggplots of the data including a histogram of the target variable (over training and test set), and some scatter plots of the relationship between the features and the target variable (you can choose which features to plot)

1. Write a function in R to create a simple neural network model with two hidden layers (see the `mnist_keras_cnn.R` for examples). Try experimenting with different layers. Create two models, one with 2 layers nad another with 6 layers. What are the differences in the architecture of these two models? Explore different layer types

1. Fit both models to the training data. What parameters do you need to specify for the training process (think about epochs, batch size, etc)? Explain the significance of the parameters you chose for the fit function. How might they affect the training process and the final performance of the model?

1. Evaluate both models on the test dataset. What metrics will you use to assess the performance of the models? How do the two models compare in terms of their performance on the test set? What might be the reasons for any observed differences?

1. Using ggplot2, create a plot to visualize the test set performance with the true test target values on the x-axis and the predicted values on the vertical axis. Can you put the predictions from both models on this plot? What insights can you draw?

1. Finally, have a think about 


The `airquality` data set is included with R. You can find a description of the data at `help(airquality)` and can get at it simply by typing `airquality` at the R command prompt. Let's suppose we're interested in predicting the variable `Ozone` from the other variables


## Task set 2: logistic regression

1. Load in the `horseshoe.csv` data set from the data directory and familiarise yourself with the data structure from the `data_description.txt` file

1. Turn the `color` and `spine` variables into factors with their proper names

1. Familiarise yourself by plotting the data and exploring the structure between the variables

1. Create a binary variable which contains only whether the satellite variable is >0 or not. We will use this as our response variable. Create a plot which shows the relationship of this variable with `width`.

1. Fit a binomial glm (a logistic regression) with the binary variable as the response and `width` as a covariate. Summarise your findings

1. Create a plot of the fitted values on top of a scatter plot of the data (hint: width on x-axis, binary response variable on y-axis)

1. Try fitting some more models to the data with more variables (and perhaps interactions) to see if you can get the AIC lower. Compare your new models' fitted values to the first model

## Task set 3: Poisson and Negative Binomial regression

1. This time fit a Poisson GLM to the horseshoe data, using the original number of satellites rather than the binary version you fitted previously. Again use `width` as the sole covariate, and again plot the fitted values

1. Now try a model with all of the covariates (make sure not to include the binary variable you created before). Summarise and see if there's any improvement. You might notice that more variables are important (compared to the previous binary logistic regression) because we're using more of the data

1. A common occurrence is that the Poisson distribution is a poor fit to the data as the mean=variance relationship is rarely met. (You could check if the mean and the variance match for these data). A common alternative is to fit a Negative-Binomial GLM which has an extra parameter to measure excess variance (over-dispersion). The `glm` function doesn't have this distribution in it by default so you need to call in the MASS library with `library(MASS)`. The command will now be called `glm.nb` which will also try to estimate the over-dispersion parameter (the excess variance over the Poisson). Use `glm.nb` to fit a Negative Binomial GLM to these data, interpret your findings, see if the AIC improves, and plot your output.

## Extra questions

If you did all the above and have time to spare, try these:

1. Another data sets which is worth fitting a linear regression model to is the `geese_isotopes.csv` data. You might like to see if one of the isotope values is affected by some of the other variables (sex, adult, etc)
2. The whitefly data set. This is binomial (as used in the lectures) but you might like to additionally try some of the other variables and see which are important and why. The data set has particular issues with zero-inflation. See if you can find which zero data points are poorly predicted by the model.

## Answers

<details>
  <summary>Task set 1</summary>
```{r ts1}
# Load necessary libraries
library(keras)
library(ggplot2)

# Part 1: Load the Boston housing data
dataset <- dataset_boston_housing()
train_data <- dataset$train$x
train_labels <- dataset$train$y
test_data <- dataset$test$x
test_labels <- dataset$test$y

# Preprocess the data
train_data <- scale(train_data)
test_data <- scale(test_data)

# Split the training data into train and validation sets (75/25 split)
set.seed(123) # for reproducibility
index <- sample(1:nrow(train_data), nrow(train_data) * 0.75)
x_train <- train_data[index,]
y_train <- train_labels[index]
x_val <- train_data[-index,]
y_val <- train_labels[-index]

# Define a function to create a CNN model
create_model <- function(layers) {
  model <- keras_model_sequential()
  model %>%
    layer_dense(units = 64, activation = 'relu', input_shape = dim(train_data)[2]) %>%
    layer_dense(units = 64, activation = 'relu')
  
  for (i in 1:layers) {
    model %>%
      layer_dense(units = 64, activation = 'relu')
  }
  
  model %>%
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = 'rmsprop', 
    loss = 'mse', 
    metrics = c('mean_absolute_error')
  )
  return(model)
}

# Create and fit the models
model_2_layers <- create_model(2)
model_6_layers <- create_model(6)

history_2_layers <- model_2_layers %>% fit(
  x_train, y_train, 
  epochs = 100, batch_size = 16, 
  validation_data = list(x_val, y_val)
)

history_6_layers <- model_6_layers %>% fit(
  x_train, y_train, 
  epochs = 100, batch_size = 16, 
  validation_data = list(x_val, y_val)
)

# Evaluate the models on the test set
score_2_layers <- model_2_layers %>% evaluate(test_data, test_labels, verbose = 0)
score_6_layers <- model_6_layers %>% evaluate(test_data, test_labels, verbose = 0)

# Prepare data for plotting
plot_data <- data.frame(
  epoch = rep(1:100, 2),
  loss = c(history_2_layers$metrics$loss, history_6_layers$metrics$loss),
  val_loss = c(history_2_layers$metrics$val_loss, history_6_layers$metrics$val_loss),
  model = rep(c("2 Layers", "6 Layers"), each = 100)
)

# Plotting
ggplot(plot_data, aes(x = epoch, y = val_loss, color = model)) +
  geom_line() +
  labs(title = "Validation Loss over Epochs", x = "Epoch", y = "Validation Loss") +
  theme_minimal()
```
</details>

<details>
  <summary>Task set 2</summary>
  ```{r ts2}
horseshoe = read.csv('../data/horseshoe.csv')
str(horseshoe)

horseshoe$color_fac = factor(horseshoe$color,
                             labels = c('light medium', 'medium', 
                                        'dark medium', 'dark'))
horseshoe$spine_fac = factor(horseshoe$spine,
                             labels = c('both good', 
                                        'one worn or broken', 
                                        'both worn or broken'))
                                        
pairs(horseshoe) # Strong relationship between width and weight
# Response variable is satell so perhaps explore more
boxplot(satell ~ spine_fac, data = horseshoe)
boxplot(satell ~ color_fac, data = horseshoe)

horseshoe$satell_bin = as.integer(horseshoe$satell > 0)
boxplot(width ~ satell_bin, data = horseshoe) # Higher width with satell > 0

logistic_mod = glm(satell_bin ~ width, data = horseshoe,
                   family = 'binomial')
summary(logistic_mod)

with(horseshoe, plot(width, satell_bin))
points(horseshoe$width, logistic_mod$fitted.values, col = 'red')
# Must be able to make this plot look nicer!

logistic_mod_2 = glm(satell_bin ~ width + color_fac + spine_fac, data = horseshoe,
                   family = 'binomial')
summary(logistic_mod_2)
AIC(logistic_mod, logistic_mod_2) # New model not any better
# Many more modelling options to try here
  ```
</details>

<details>
  <summary>Task set 3</summary>
  ```{r ts3}
pois_mod = glm(satell ~ width, data = horseshoe, family = poisson)
summary(pois_mod)

# Example of a qq-plot for a Poisson distribution - bit of a hack!
# qqrplot comes from the countreg package
source('https://raw.githubusercontent.com/rforge/countreg/master/pkg/R/qqrplot.R')
source('https://raw.githubusercontent.com/rforge/countreg/master/pkg/R/qresiduals.R')
qqrplot(pois_mod, 
        main = "Poisson")
        
pois_mod_2 = glm(satell ~ width + color_fac + spine_fac + weight, data = horseshoe, family = poisson)
summary(pois_mod_2)

library(MASS)
nb_mod = glm.nb(satell ~ width + color_fac + spine_fac + weight, data = horseshoe)
summary(nb_mod)
AIC(pois_mod, pois_mod_2, nb_mod) # NegBin model way 'better' in terms of AIC
# Plot of fitted values
with(horseshoe, plot(width, satell))
points(horseshoe$width, nb_mod$fitted.values, col = 'red')
# Width not the only variable we could plot against (try e.g. color_fac or others)

# QQ-plot for negative binomial
qqrplot(nb_mod, 
        main = "Negative Binomial")
# Compare with the Poisson fit
qqrplot(pois_mod_2, 
        main = "Poisson")
  ```
</details>


